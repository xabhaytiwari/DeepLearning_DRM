{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-1w_dlfWBKq",
        "outputId": "4a364f8e-6586-46c2-ddda-1e1d4b04d9be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Imports, double precision, and device\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle, time\n",
        "\n",
        "torch.set_default_dtype(torch.float64)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZZmHAQlWHW-",
        "outputId": "b0c4b454-83d7-415a-8fb7-f67fde257841"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Net(\n",
              "  (layers): Sequential(\n",
              "    (0): Linear(in_features=2, out_features=100, bias=True)\n",
              "    (1): Tanh()\n",
              "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (3): Tanh()\n",
              "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (5): Tanh()\n",
              "    (6): Linear(in_features=100, out_features=100, bias=True)\n",
              "    (7): Tanh()\n",
              "    (8): Linear(in_features=100, out_features=1, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "# Cell 2: Big, deep NN\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(2, 100), nn.Tanh(),\n",
        "            nn.Linear(100, 100), nn.Tanh(),\n",
        "            nn.Linear(100, 100), nn.Tanh(),\n",
        "            nn.Linear(100, 100), nn.Tanh(),\n",
        "            nn.Linear(100, 1)\n",
        "        )\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "model = Net().to(device)\n",
        "def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "        nn.init.xavier_uniform_(m.weight)\n",
        "        nn.init.zeros_(m.bias)\n",
        "model.apply(init_weights)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "iEl63ofZWHT8"
      },
      "outputs": [],
      "source": [
        "def exact_solution(x):\n",
        "    # x: [N,2]\n",
        "    # u(x1, x2) = x1^2 * x2^2 * (1 - x1)^2 * (1 - x2)^2\n",
        "    return x[:,0]**2 * x[:,1]**2 * (1 - x[:,0])**2 * (1 - x[:,1])**2\n",
        "\n",
        "def rhs_f(x):\n",
        "    x.requires_grad_()\n",
        "    u = exact_solution(x)\n",
        "\n",
        "    # 1. First Laplacian (Delta u)\n",
        "    grad_u = torch.autograd.grad(u, x, torch.ones_like(u), create_graph=True)[0]\n",
        "    lap_u = sum([\n",
        "        torch.autograd.grad(grad_u[:, i], x, torch.ones_like(grad_u[:, i]), create_graph=True)[0][:, i]\n",
        "        for i in range(2)\n",
        "    ])\n",
        "\n",
        "    # 2. Second Laplacian (Delta^2 u) - Apply same logic to lap_u\n",
        "    grad_lap = torch.autograd.grad(lap_u, x, torch.ones_like(lap_u), create_graph=True)[0]\n",
        "    biharmonic = sum([\n",
        "        torch.autograd.grad(grad_lap[:, i], x, torch.ones_like(grad_lap[:, i]), create_graph=True)[0][:, i]\n",
        "        for i in range(2)\n",
        "    ])\n",
        "\n",
        "    return biharmonic.detach() # Detach because f is a constant constant ground truth\n",
        "def dirichlet_bc(x):  # Dirichlet on all boundaries\n",
        "    return exact_solution(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tbzeoC9uWHRn"
      },
      "outputs": [],
      "source": [
        "# Cell 4: Boundary sampler with normals\n",
        "N_int, N_bd = 10000, 4000\n",
        "\n",
        "def get_interior(N):\n",
        "    return torch.rand(N, 2, device=device)\n",
        "\n",
        "def get_boundary(M):\n",
        "    grid = torch.linspace(0, 1, M//4, device=device)\n",
        "    zeros = torch.zeros_like(grid)\n",
        "    ones = torch.ones_like(grid)\n",
        "\n",
        "    # Points on the 4 sides\n",
        "    pts = [\n",
        "        torch.stack([grid, zeros], dim=1), # Bottom (y=0)\n",
        "        torch.stack([grid, ones], dim=1),  # Top (y=1)\n",
        "        torch.stack([zeros, grid], dim=1), # Left (x=0)\n",
        "        torch.stack([ones, grid], dim=1)   # Right (x=1)\n",
        "    ]\n",
        "\n",
        "    # Corresponding Normal vectors\n",
        "    normals = [\n",
        "        torch.stack([zeros, -ones], dim=1), # Bottom normal (0, -1)\n",
        "        torch.stack([zeros, ones], dim=1),  # Top normal (0, 1)\n",
        "        torch.stack([-ones, zeros], dim=1), # Left normal (-1, 0)\n",
        "        torch.stack([ones, zeros], dim=1)   # Right normal (1, 0)\n",
        "    ]\n",
        "\n",
        "    return torch.cat(pts, dim=0), torch.cat(normals, dim=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3R1jz7GFWHPX"
      },
      "outputs": [],
      "source": [
        "# Cell 5: DRM Loss with Neumann Condition\n",
        "def drm_loss(model, x_int, x_bd, n_bd, bc_weight=100.0):\n",
        "    # --- 1. Domain Energy (Interior) ---\n",
        "    x_int.requires_grad_()\n",
        "    u = model(x_int)\n",
        "\n",
        "    # Compute Laplacian via Autograd\n",
        "    grad_u = torch.autograd.grad(u, x_int, torch.ones_like(u), create_graph=True)[0]\n",
        "    lap_u = sum([\n",
        "        torch.autograd.grad(grad_u[:, i], x_int, torch.ones_like(grad_u[:, i]), create_graph=True)[0][:, i]\n",
        "        for i in range(2)\n",
        "    ])\n",
        "\n",
        "    # Compute Biharmonic (Laplacian of Laplacian)\n",
        "    # grad_lap = torch.autograd.grad(lap_u, x_int, grad_outputs=torch.ones_like(lap_u), create_graph=True)[0]\n",
        "    # biharmonic = sum([\n",
        "    #    torch.autograd.grad(grad_lap[:, i], x_int, torch.ones_like(grad_lap[:, i]), create_graph=True)[0][:, i]\n",
        "    #    for i in range(2)\n",
        "    # ])\n",
        "\n",
        "    f = rhs_f(x_int)\n",
        "    energy = ((0.5 * lap_u ** 2 - f * u.squeeze()).mean())\n",
        "\n",
        "    # --- 2. Boundary Conditions ---\n",
        "    # We need gradients on boundary points to compute normal derivative\n",
        "    x_bd.requires_grad_()\n",
        "    u_bd = model(x_bd).squeeze()\n",
        "\n",
        "    # A. Dirichlet Loss (u = 0 on boundary)\n",
        "    # Note: dirichlet_bc(x_bd) returns 0 for this exact solution\n",
        "    bc_dir_loss = ((u_bd - dirichlet_bc(x_bd))**2).mean()\n",
        "\n",
        "   # B. Neumann Target (g2 = du_exact / dn)\n",
        "    # -- Calculate Exact Normal Derivative --\n",
        "    u_exact_bd = exact_solution(x_bd)\n",
        "    grad_exact_bd = torch.autograd.grad(u_exact_bd, x_bd, torch.ones_like(u_exact_bd), create_graph=True)[0]\n",
        "    target_neumann = (grad_exact_bd * n_bd).sum(dim=1).detach() # This will be 0.0 for this problem\n",
        "\n",
        "    # -- Calculate Predicted Normal Derivative --\n",
        "    grad_u_bd = torch.autograd.grad(u_bd, x_bd, torch.ones_like(u_bd), create_graph=True)[0]\n",
        "    du_dn_pred = (grad_u_bd * n_bd).sum(dim=1)\n",
        "\n",
        "    # -- Neumann Loss --\n",
        "    bc_neu_loss = ((du_dn_pred - target_neumann)**2).mean()\n",
        "\n",
        "    return energy + bc_weight * (bc_dir_loss + bc_neu_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3Dm9qVuWRSA",
        "outputId": "b6a1e60b-0935-4780-ef65-bcacf6f69f28"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss 2.966447e+00\n",
            "Epoch 500: Loss -2.821252e-03\n",
            "Epoch 1000: Loss -3.371073e-03\n",
            "Epoch 1500: Loss -6.827790e-03\n",
            "Epoch 2000: Loss -6.547862e-03\n",
            "Epoch 2500: Loss -6.590160e-03\n",
            "Epoch 3000: Loss -6.677724e-03\n",
            "Epoch 3500: Loss -6.581500e-03\n",
            "Epoch 4000: Loss -6.648561e-03\n",
            "Epoch 4500: Loss -6.772307e-03\n"
          ]
        }
      ],
      "source": [
        "# Cell 6: Training\n",
        "epochs = 12000\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5000, gamma=0.3)\n",
        "losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Unpack normals here\n",
        "    x_int, (x_bd, n_bd) = get_interior(N_int), get_boundary(N_bd)\n",
        "\n",
        "    # Pass normals to loss\n",
        "    loss = drm_loss(model, x_int, x_bd, n_bd, bc_weight=100.0)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.5)\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss {loss.item():.6e}\")\n",
        "\n",
        "# Final LBFGS\n",
        "lbfgs = torch.optim.LBFGS(model.parameters(), lr=0.5, max_iter=250)\n",
        "def closure():\n",
        "    x_int, (x_bd, n_bd) = get_interior(N_int), get_boundary(N_bd)\n",
        "    loss = drm_loss(model, x_int, x_bd, n_bd, bc_weight=4000.0)\n",
        "    lbfgs.zero_grad()\n",
        "    loss.backward()\n",
        "    return loss\n",
        "lbfgs.step(closure)\n",
        "train_time = time.time() - start_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wjBz_hyWNQj"
      },
      "outputs": [],
      "source": [
        "# Cell 7: Save outputa\n",
        "torch.save(model.state_dict(), \"drm_model_q2.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlHDZKHTWNNZ"
      },
      "outputs": [],
      "source": [
        "# Cell 8: Print arch/settings\n",
        "print(f\"Training time: {train_time:.2f} s\")\n",
        "print(\"NN Architecture:\\n\", model)\n",
        "n_L = sum(torch.count_nonzero(p).item() for p in model.parameters())\n",
        "print(\"Total nonzero parameters n_L:\", n_L)\n",
        "print(\"Adam lr: 5e-4, LBFGS steps: 250\")\n",
        "print(\"Interior pts:\", N_int, \"Boundary pts:\", N_bd, \"Boundary penalty:\", 100.0)\n",
        "print(f\"Final training loss: {losses[-1]:.6e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rFeiCzoWWNLK"
      },
      "outputs": [],
      "source": [
        "# Cell 9: Plot solution/error\n",
        "gsize = 61\n",
        "xg = np.linspace(0, 1, gsize)\n",
        "mg = np.meshgrid(xg, xg)\n",
        "gp = torch.tensor(np.stack([mg[0].ravel(), mg[1].ravel()], axis=-1), device=device, dtype=torch.float64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    u_nn = model(gp).detach().cpu().numpy().reshape(gsize, gsize)\n",
        "    u_gt = exact_solution(gp).detach().cpu().numpy().reshape(gsize, gsize)\n",
        "    err = np.abs(u_nn - u_gt)\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(mg[0], mg[1], u_nn, cmap='jet')\n",
        "plt.title('NN Solution')\n",
        "plt.savefig(\"NNsolution2.png\")\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(mg[0], mg[1], u_gt, cmap='jet')\n",
        "plt.title('GT Solution')\n",
        "plt.savefig(\"GTsol2.png\")\n",
        "plt.show()\n",
        "\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.plot_surface(mg[0], mg[1], err, cmap='jet')\n",
        "plt.title('|u-u_theta| Error')\n",
        "plt.savefig(\"Error2.png\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zQCLhqwBWNJA"
      },
      "outputs": [],
      "source": [
        "## Cell 10: Error calculation (careful detaching!)\n",
        "gp.requires_grad_()\n",
        "u_gt1d = exact_solution(gp).detach().cpu().numpy()\n",
        "u_nn1d = model(gp).detach().cpu().numpy()\n",
        "\n",
        "u_gt_val = exact_solution(gp)\n",
        "u_nn_val = model(gp)\n",
        "u_gt_grad = torch.autograd.grad(u_gt_val, gp, torch.ones_like(u_gt_val), create_graph=True)[0]\n",
        "u_nn_grad = torch.autograd.grad(u_nn_val, gp, torch.ones_like(u_nn_val), create_graph=True)[0]\n",
        "u_gt_grad_np = u_gt_grad.detach().cpu().numpy()\n",
        "u_nn_grad_np = u_nn_grad.detach().cpu().numpy()\n",
        "H_gt = [torch.autograd.grad(u_gt_grad[:, i], gp, torch.ones_like(u_gt_grad[:, i]), create_graph=True)[0][:, i].detach().cpu().numpy() for i in range(2)]\n",
        "H_nn = [torch.autograd.grad(u_nn_grad[:, i], gp, torch.ones_like(u_nn_grad[:, i]), create_graph=True)[0][:, i].detach().cpu().numpy() for i in range(2)]\n",
        "def H2norm(H): return np.sqrt(sum([np.mean(h**2) for h in H]))\n",
        "def H1norm(grad): return np.sqrt(np.mean(grad[:,0]**2 + grad[:,1]**2))\n",
        "\n",
        "L2Err = np.sqrt(np.mean((u_gt1d-u_nn1d)**2))\n",
        "RelL2Err = L2Err / np.sqrt(np.mean(u_gt1d**2))\n",
        "energyErr = L2Err + H1norm(u_gt_grad_np-u_nn_grad_np)\n",
        "relEnergyErr = energyErr / (np.sqrt(np.mean(u_gt1d**2)) + H1norm(u_gt_grad_np))\n",
        "H2Err = H2norm([H_gt[0]-H_nn[0], H_gt[1]-H_nn[1]])\n",
        "relH2Err = H2Err / H2norm(H_gt)\n",
        "\n",
        "print(f\"L2 Error: {L2Err:.3e}\")\n",
        "print(f\"Relative L2 Error: {RelL2Err:.3e}\")\n",
        "print(f\"Energy Error (L2 + H1): {energyErr:.3e}\")\n",
        "print(f\"Relative Energy Error (H1Relative): {relEnergyErr:.3e}\")\n",
        "print(f\"H2 Error: {H2Err:.3e}\")\n",
        "print(f\"Relative H2 Error: {relH2Err:.3e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iEuy-TqCWd31"
      },
      "outputs": [],
      "source": [
        "plt.figure()\n",
        "plt.plot(losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.yscale('log')  # <- THIS is the key change!\n",
        "plt.title(\"Training Loss History (Log Scale)\")\n",
        "plt.savefig(\"LossHistory_log_2.png\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}